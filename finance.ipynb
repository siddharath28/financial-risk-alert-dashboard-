{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b52c5b-87ce-40ff-bfd9-0231ebc5538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or overwrite the .env file\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(\"NEWS_API_KEY=468c0aaedd694cdf905ed97c1dcdc0b4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef727d4-0387-40a2-a499-6dcd69432730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API KEY: 468c0aaedd694cdf905ed97c1dcdc0b4\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads .env file from current directory\n",
    "\n",
    "print(\"API KEY:\", os.getenv(\"NEWS_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1940e70-b423-4f78-80f9-37dd10f1def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the 'data' folder if it doesn't already exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3897d6e4-4084-40f4-987d-4d36fcdc3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('NEWS_API_KEY')\n",
    "\n",
    "# Function to fetch news\n",
    "def fetch_news(query=\"stock market\", from_date=\"2024-05-01\", to_date=\"2024-05-07\", page_size=100):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    all_articles = []\n",
    "\n",
    "    # Only one page allowed on free plan\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'from': from_date,\n",
    "        'to': to_date,\n",
    "        'language': 'en',\n",
    "        'sortBy': 'relevancy',\n",
    "        'pageSize': page_size,\n",
    "        'page': 1,  # Only one page\n",
    "        'apiKey': API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch:', response.json())\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = response.json()\n",
    "    articles = data.get('articles', [])\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "    return pd.DataFrame(all_articles)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991ba77-4190-4723-8660-212db2909275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acca74b2-db07-4cd7-a6a8-e959f42382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news(query_list, from_date='2024-05-01', to_date='2024-05-07', page_size=100):\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "    all_articles = []\n",
    "    \n",
    "    for query in query_list:\n",
    "        print(f\"Fetching news for query: {query}\")\n",
    "        \n",
    "        params = {\n",
    "            'q': query,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'language': 'en',\n",
    "            'sortBy': 'relevancy',\n",
    "            'pageSize': page_size,\n",
    "            'page': 1,  # Only one page for each query\n",
    "            'apiKey': API_KEY\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed to fetch for query: {query}, response: {response.json()}')\n",
    "            continue  # Skip this query if it fails\n",
    "\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        all_articles.extend(articles)\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7d4d48-bf8d-4679-be0a-e118e4d9aff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'week_ago' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock market\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minflation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNASDAQ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinancial news\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m df_news \u001b[38;5;241m=\u001b[39m fetch_news(query_list, from_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(week_ago), to_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(today))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_news\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m     df_news \u001b[38;5;241m=\u001b[39m df_news[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishedAt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'week_ago' is not defined"
     ]
    }
   ],
   "source": [
    "query_list = [\"stock market\", \"inflation\", \"NASDAQ\", \"financial news\", \"economy\"]\n",
    "df_news = fetch_news(query_list, from_date=str(week_ago), to_date=str(today))\n",
    "\n",
    "if not df_news.empty:\n",
    "    df_news = df_news[[\"publishedAt\", \"title\", \"description\", \"url\", \"source\"]]\n",
    "    df_news.to_csv(\"data/financial_news.csv\", index=False)\n",
    "    df_news.head()\n",
    "else:\n",
    "    print(\"No articles fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac61ddf0-71e2-40c2-bfa7-68be14857e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the data into a CSV file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df_news\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/financial_news.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check the first few rows of the dataframe\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_news\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_news' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df_news.to_csv(\"data/financial_news.csv\", index=False)\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "df_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e254f80-fda3-4631-9823-aa42f7048b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psaw in c:\\users\\sandi\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from psaw) (2.32.3)\n",
      "Requirement already satisfied: Click in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from psaw) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from Click->psaw) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests->psaw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests->psaw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests->psaw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests->psaw) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install psaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322719ea-9c6b-4b23-9e57-a557b1425178",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_content = \"\"\"REDDIT_CLIENT_ID=mNM6bsmfJfWlE24n1skAOg\n",
    "REDDIT_CLIENT_SECRET=VldZScwuDTfLYsGpEw1MpRD9-t-lBQ\n",
    "REDDIT_USER_AGENT=financial_news_script\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\".env file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55a761-88ed-43b7-a3b6-5d91584c25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Client ID:\", os.getenv(\"REDDIT_CLIENT_ID\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8511ee-bb3f-4526-8d7d-52cfe16c2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546c627-7efa-4bc2-8b62-991e3da97ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb76e-e83c-4923-98b7-44bb1a14eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "subreddits = [\"StockMarket\", \"investing\", \"wallstreetbets\"]\n",
    "posts = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    for post in reddit.subreddit(sub).top(time_filter='week', limit=100):\n",
    "        posts.append({\n",
    "            'subreddit': sub,\n",
    "            'title': post.title,\n",
    "            'description': post.selftext,\n",
    "            'url': post.url,\n",
    "            'publishedAt': datetime.utcfromtimestamp(post.created_utc),\n",
    "            'source': 'Reddit'\n",
    "        })\n",
    "\n",
    "df_reddit = pd.DataFrame(posts)\n",
    "df_reddit.to_csv(\"data/reddit_financial_news.csv\", index=False)\n",
    "df_reddit.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0ee9a-a410-4570-908b-9d43018c3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_yahoo_finance_news():\n",
    "    url = \"https://finance.yahoo.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    headlines = []\n",
    "    for item in soup.select(\"li.js-stream-content\"):\n",
    "        title_tag = item.find(\"h3\")\n",
    "        link_tag = item.find(\"a\")\n",
    "        if title_tag and link_tag:\n",
    "            headlines.append({\n",
    "                \"title\": title_tag.text.strip(),\n",
    "                \"url\": \"https://finance.yahoo.com\" + link_tag[\"href\"],\n",
    "                \"publishedAt\": datetime.now(),\n",
    "                \"description\": \"\",\n",
    "                \"source\": \"Yahoo Finance\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(headlines)\n",
    "\n",
    "# Run it\n",
    "df_yahoo = scrape_yahoo_finance_news()\n",
    "df_yahoo.to_csv(\"data/yahoo_financial_news.csv\", index=False)\n",
    "df_yahoo.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc2baa-a433-40a3-9f88-89eef612f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f184502-a6fe-46ea-9492-c5374745975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_yahoo_finance_rss():\n",
    "    rss_url = \"https://finance.yahoo.com/news/rssindex\"\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    news_items = []\n",
    "    for entry in feed.entries:\n",
    "        news_items.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"publishedAt\": datetime(*entry.published_parsed[:6]),\n",
    "            \"description\": entry.get(\"summary\", \"\"),\n",
    "            \"source\": \"Yahoo Finance\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(news_items)\n",
    "\n",
    "# Run and save\n",
    "df_yahoo_rss = fetch_yahoo_finance_rss()\n",
    "df_yahoo_rss.to_csv(\"data/yahoo_financial_news.csv\", index=False)\n",
    "df_yahoo_rss.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc6e2e-57cd-44b6-83b5-f104c3935597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df_reddit = pd.read_csv(\"data/reddit_financial_news.csv\")\n",
    "df_yahoo = pd.read_csv(\"data/yahoo_financial_news.csv\")\n",
    "\n",
    "# Ensure consistent column names\n",
    "common_cols = [\"title\", \"url\", \"publishedAt\", \"description\", \"source\"]\n",
    "df_reddit = df_reddit[common_cols]\n",
    "df_yahoo = df_yahoo[common_cols]\n",
    "\n",
    "# Merge datasets\n",
    "df_all = pd.concat([df_reddit, df_yahoo], ignore_index=True)\n",
    "\n",
    "# Drop duplicates (if any)\n",
    "df_all.drop_duplicates(subset=[\"title\", \"url\"], inplace=True)\n",
    "\n",
    "# Save merged file\n",
    "df_all.to_csv(\"data/all_financial_news.csv\", index=False)\n",
    "\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9951e3f-08b2-403f-ab27-486793f708fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the financial_news.csv dataset (if available)\n",
    "df_financial_news = pd.read_csv(\"data/financial_news.csv\")\n",
    "\n",
    "# Ensure the column names are consistent across all datasets\n",
    "df_financial_news = df_financial_news[common_cols]\n",
    "\n",
    "# Merge everything into one final dataset\n",
    "df_all = pd.concat([df_reddit, df_yahoo, df_financial_news], ignore_index=True)\n",
    "\n",
    "# Drop duplicates based on title and URL\n",
    "df_all.drop_duplicates(subset=[\"title\", \"url\"], inplace=True)\n",
    "\n",
    "# Save the final merged dataset\n",
    "df_all.to_csv(\"data/all_financial_news.csv\", index=False)\n",
    "\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de413be6-1d2c-4bd6-add4-71c1c12d662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade45898-688f-48c2-b98d-5b21c97b7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a4c59-f83d-467d-870e-82e16be082c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Combine title + description for context\n",
    "df_all[\"text\"] = df_all[\"title\"].fillna(\"\") + \". \" + df_all[\"description\"].fillna(\"\")\n",
    "\n",
    "# Initialize analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment scoring\n",
    "df_all[\"sentiment_score\"] = df_all[\"text\"].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
    "df_all[\"sentiment_label\"] = df_all[\"sentiment_score\"].apply(\n",
    "    lambda score: \"positive\" if score > 0.05 else (\"negative\" if score < -0.05 else \"neutral\")\n",
    ")\n",
    "\n",
    "# Save results\n",
    "df_all.to_csv(\"data/news_with_sentiment.csv\", index=False)\n",
    "df_all[[\"publishedAt\", \"source\", \"title\", \"sentiment_label\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25847e-e226-45aa-8963-635e9257d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically infer datetime format and handle errors\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "df_all['publishedAt'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1733b5e-639c-4695-96dd-faff2518a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically infer datetime format and handle errors\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "df_all['publishedAt'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6ba93-0293-4334-9222-92b10ce85563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Z' (UTC timezone) if present and convert to datetime\n",
    "df_all['publishedAt'] = df_all['publishedAt'].str.replace('Z', '', regex=False)\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "df_all['publishedAt'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccf6b9-9e2b-427e-8309-5036f4ccbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_all = pd.read_csv(\"data/all_financial_news.csv\")\n",
    "\n",
    "# Fix datetime parsing by allowing mixed or ISO formats\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce', format='ISO8601')\n",
    "\n",
    "# Drop rows where datetime conversion failed\n",
    "df_all.dropna(subset=['publishedAt'], inplace=True)\n",
    "\n",
    "# Sort values\n",
    "df_all.sort_values(by='publishedAt', ascending=True, inplace=True)\n",
    "\n",
    "df_all['publishedAt'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f220ae-52a9-49d6-ac78-1bffc8813e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df_all = pd.read_csv(\"data/all_financial_news.csv\")\n",
    "\n",
    "# Convert to datetime in UTC to make all timestamps tz-aware\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce', utc=True)\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df_all.dropna(subset=['publishedAt'], inplace=True)\n",
    "\n",
    "# Sort chronologically\n",
    "df_all.sort_values(by='publishedAt', ascending=True, inplace=True)\n",
    "\n",
    "# Preview\n",
    "df_all['publishedAt'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59430b-cf2f-403a-94a9-daccf7d0dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6228417-56eb-474a-aa36-8189c21cbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime format (force conversion)\n",
    "df_all['publishedAt'] = pd.to_datetime(df_all['publishedAt'], errors='coerce', utc=True)\n",
    "\n",
    "# Drop rows where datetime conversion failed\n",
    "df_all.dropna(subset=['publishedAt'], inplace=True)\n",
    "\n",
    "# Now extract the date\n",
    "df_all['date'] = df_all['publishedAt'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a58d79-517e-42d8-b52a-44cb9d4cba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date to get daily average sentiment\n",
    "daily_sentiment = df_all.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Add risk alert flag based on threshold\n",
    "risk_threshold = -0.3\n",
    "daily_sentiment['risk_alert'] = daily_sentiment['sentiment_score'] < risk_threshold\n",
    "\n",
    "# Display risky days\n",
    "daily_sentiment[daily_sentiment['risk_alert']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d0af3-7b98-4c54-b7f0-2cef87434575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define risk threshold\n",
    "risk_threshold = -0.3\n",
    "\n",
    "# Add risk alert flag\n",
    "daily_sentiment['risk_alert'] = daily_sentiment['sentiment_score'] < risk_threshold\n",
    "\n",
    "# Show risky days\n",
    "risky_days = daily_sentiment[daily_sentiment['risk_alert'] == True]\n",
    "risky_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f385b8f5-a4c5-4253-b59b-184465dfad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(daily_sentiment['date'], daily_sentiment['sentiment_score'], marker='o')\n",
    "plt.axhline(y=risk_threshold, color='red', linestyle='--', label='Risk Threshold')\n",
    "plt.title(\"Daily Average Financial Sentiment\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5e0b5-e894-4c1c-9b80-403aba2fc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a 'date' column (drop the time part)\n",
    "df_all['date'] = df_all['publishedAt'].dt.date\n",
    "\n",
    "# Step 2: Aggregate sentiment per day (mean sentiment)\n",
    "daily_sentiment = df_all.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Step 3: Calculate a 3-day rolling average\n",
    "daily_sentiment['rolling_sentiment'] = daily_sentiment['sentiment_score'].rolling(window=3).mean()\n",
    "\n",
    "# Step 4: Define risk levels based on thresholds\n",
    "def classify_risk(score):\n",
    "    if score < -0.2:\n",
    "        return 'ğŸ”´ High Risk'\n",
    "    elif score < 0:\n",
    "        return 'ğŸŸ  Medium Risk'\n",
    "    else:\n",
    "        return 'ğŸŸ¢ Low Risk'\n",
    "\n",
    "daily_sentiment['risk_level'] = daily_sentiment['rolling_sentiment'].apply(classify_risk)\n",
    "\n",
    "# Show the results\n",
    "daily_sentiment.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19da470-b25a-4bf5-b0ab-78db736bb0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure 'daily_sentiment' has 'sentiment_score' and 'date'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(daily_sentiment\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define a risk threshold\u001b[39;00m\n\u001b[0;32m      5\u001b[0m risk_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure 'daily_sentiment' has 'sentiment_score' and 'date'\n",
    "print(daily_sentiment.columns)\n",
    "\n",
    "# Define a risk threshold\n",
    "risk_threshold = -0.2\n",
    "\n",
    "# Create the 'risk_alert' column safely\n",
    "if 'sentiment_score' in daily_sentiment.columns:\n",
    "    daily_sentiment['risk_alert'] = daily_sentiment['sentiment_score'] < risk_threshold\n",
    "else:\n",
    "    raise ValueError(\"Column 'sentiment_score' not found in daily_sentiment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eba3af-4f84-457d-ae03-a8cdcbc71dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot daily sentiment\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(daily_sentiment['date'], daily_sentiment['sentiment_score'], marker='o', label='Sentiment Score')\n",
    "\n",
    "# Highlight risk alert days\n",
    "risk_days = daily_sentiment[daily_sentiment['risk_alert']]\n",
    "plt.scatter(risk_days['date'], risk_days['sentiment_score'], color='red', label='Risk Alert', zorder=5)\n",
    "\n",
    "# Plot formatting\n",
    "plt.axhline(y=risk_threshold, color='gray', linestyle='--', label='Risk Threshold')\n",
    "plt.title('Daily Financial News Sentiment with Risk Alerts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec3535fe-c286-48d5-bf14-0ff7f0a002fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252cf596-a323-4655-a9b7-8eb8c625442b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
